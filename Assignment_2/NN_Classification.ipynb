{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e105a6b9",
   "metadata": {},
   "source": [
    "### Neural Network for Dataset 2 (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2342,
   "id": "c6428f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2343,
   "id": "d0d0a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE DATASET\n",
    "\n",
    "# 1. Regression\n",
    "# diabetes = datasets.load_diabetes(as_frame=True)\n",
    "# X = diabetes.data.values\n",
    "# y = diabetes.target.values\n",
    "\n",
    "# 2. Classification\n",
    "diabetes = datasets.fetch_openml(name=\"diabetes\", version=1, as_frame=True)\n",
    "X = diabetes.data.values\n",
    "y = diabetes.target.astype(str).map({'tested_positive': 1, 'tested_negative': 0}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2344,
   "id": "aafa6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test spliting\n",
    "test_size=0.2\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2345,
   "id": "2dfc9566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler=StandardScaler()\n",
    "Xtr= scaler.fit_transform(Xtr)\n",
    "Xte= scaler.transform(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56f925b",
   "metadata": {},
   "source": [
    "### NN Architecture\n",
    "The architecture can be tuned by changing the number of layers, layer size and regularization (dropout). Dropout prevents overfitting by randomly \"dropping out\" (setting to zero) a fraction of the neurons during training, which forces the network to learn more robust and generalized features. Regularization is determined later together with other hyperparameters.  \n",
    "  \n",
    "It can be very hard to determine an optimal architecture but I ended up using the following:  \n",
    "**Regression**: 3 hidden layers with 64 neurons each.  \n",
    "**Classification**: 4 hidden layers with 64 neurons in the first three and 32 neurons in the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2346,
   "id": "99c0966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size=1, dropout_prob=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.out = nn.Linear(32, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac455b",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "- **`num_epochs`** – Number of training passes over the entire dataset. Don't want too many epochs to avoid overfitting to noise.  \n",
    "- **`lr`** – Learning rate. Step size for updating weights during training. Controls how fast the model learns.  \n",
    "- **`dropout`** – Fraction of neurons randomly dropped during training to reduce overfitting. In this task I'm using 10% but for toy datasets, higher fraction could be used.  \n",
    "- **`batch_size`** – Number of samples processed before updating the model. If too much RAM is being used, this value could for example be dropped to 32.  \n",
    "  \n",
    "After tuning the model, the following values were chosen for the different datasets:  \n",
    "**Regression**: `num_epochs`=60, `lr`=0.001, `dropout`=0.1, `batch_size`=64  \n",
    "**Classification**: `num_epochs`=75, `lr`=0.001, `dropout`=0.1, `batch_size`=64  \n",
    "  \n",
    "Important to note is that these values are not necessarily fully optimized. Training the model over and over with the same hyperparameters can give very different performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2347,
   "id": "2ab972de",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=75 \n",
    "lr=0.001\n",
    "dropout=0.1\n",
    "batch_size=64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2348,
   "id": "49298ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = torch.tensor(Xtr, dtype=torch.float32)\n",
    "ytr = torch.tensor(ytr, dtype=torch.float32)\n",
    "Xte = torch.tensor(Xte, dtype=torch.float32)\n",
    "yte = torch.tensor(yte, dtype=torch.float32)\n",
    "\n",
    "# Wrap Xtr and ytr into a dataset\n",
    "train_dataset = TensorDataset(Xtr, ytr)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2349,
   "id": "8e649c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Loss, Optimizer\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "# Ignoring this line since I'm not using cuda\n",
    "\n",
    "model = MLP(input_size=Xtr.shape[1], dropout_prob=dropout)#.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # for binary classification\n",
    "# criterion = nn.MSELoss() # for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) #can use different optimizer such as AdamW but not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2350,
   "id": "e02dec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/75], Loss: 0.6835\n",
      "Epoch [2/75], Loss: 0.6692\n",
      "Epoch [3/75], Loss: 0.6394\n",
      "Epoch [4/75], Loss: 0.5882\n",
      "Epoch [5/75], Loss: 0.5221\n",
      "Epoch [6/75], Loss: 0.4957\n",
      "Epoch [7/75], Loss: 0.4754\n",
      "Epoch [8/75], Loss: 0.4677\n",
      "Epoch [9/75], Loss: 0.4749\n",
      "Epoch [10/75], Loss: 0.4667\n",
      "Epoch [11/75], Loss: 0.4353\n",
      "Epoch [12/75], Loss: 0.4642\n",
      "Epoch [13/75], Loss: 0.4428\n",
      "Epoch [14/75], Loss: 0.4389\n",
      "Epoch [15/75], Loss: 0.4491\n",
      "Epoch [16/75], Loss: 0.4447\n",
      "Epoch [17/75], Loss: 0.4344\n",
      "Epoch [18/75], Loss: 0.4383\n",
      "Epoch [19/75], Loss: 0.4351\n",
      "Epoch [20/75], Loss: 0.4278\n",
      "Epoch [21/75], Loss: 0.4274\n",
      "Epoch [22/75], Loss: 0.4197\n",
      "Epoch [23/75], Loss: 0.4188\n",
      "Epoch [24/75], Loss: 0.4240\n",
      "Epoch [25/75], Loss: 0.4137\n",
      "Epoch [26/75], Loss: 0.4202\n",
      "Epoch [27/75], Loss: 0.4215\n",
      "Epoch [28/75], Loss: 0.4141\n",
      "Epoch [29/75], Loss: 0.4112\n",
      "Epoch [30/75], Loss: 0.4078\n",
      "Epoch [31/75], Loss: 0.3989\n",
      "Epoch [32/75], Loss: 0.4067\n",
      "Epoch [33/75], Loss: 0.4010\n",
      "Epoch [34/75], Loss: 0.3892\n",
      "Epoch [35/75], Loss: 0.4026\n",
      "Epoch [36/75], Loss: 0.3921\n",
      "Epoch [37/75], Loss: 0.3977\n",
      "Epoch [38/75], Loss: 0.4073\n",
      "Epoch [39/75], Loss: 0.3947\n",
      "Epoch [40/75], Loss: 0.3839\n",
      "Epoch [41/75], Loss: 0.3896\n",
      "Epoch [42/75], Loss: 0.3842\n",
      "Epoch [43/75], Loss: 0.3847\n",
      "Epoch [44/75], Loss: 0.3763\n",
      "Epoch [45/75], Loss: 0.3743\n",
      "Epoch [46/75], Loss: 0.3690\n",
      "Epoch [47/75], Loss: 0.3823\n",
      "Epoch [48/75], Loss: 0.3699\n",
      "Epoch [49/75], Loss: 0.3629\n",
      "Epoch [50/75], Loss: 0.3632\n",
      "Epoch [51/75], Loss: 0.3817\n",
      "Epoch [52/75], Loss: 0.3681\n",
      "Epoch [53/75], Loss: 0.3643\n",
      "Epoch [54/75], Loss: 0.3665\n",
      "Epoch [55/75], Loss: 0.3735\n",
      "Epoch [56/75], Loss: 0.3527\n",
      "Epoch [57/75], Loss: 0.3486\n",
      "Epoch [58/75], Loss: 0.3389\n",
      "Epoch [59/75], Loss: 0.3513\n",
      "Epoch [60/75], Loss: 0.3435\n",
      "Epoch [61/75], Loss: 0.3503\n",
      "Epoch [62/75], Loss: 0.3508\n",
      "Epoch [63/75], Loss: 0.3630\n",
      "Epoch [64/75], Loss: 0.3472\n",
      "Epoch [65/75], Loss: 0.3382\n",
      "Epoch [66/75], Loss: 0.3241\n",
      "Epoch [67/75], Loss: 0.3291\n",
      "Epoch [68/75], Loss: 0.3372\n",
      "Epoch [69/75], Loss: 0.3528\n",
      "Epoch [70/75], Loss: 0.3269\n",
      "Epoch [71/75], Loss: 0.3217\n",
      "Epoch [72/75], Loss: 0.3323\n",
      "Epoch [73/75], Loss: 0.3386\n",
      "Epoch [74/75], Loss: 0.3416\n",
      "Epoch [75/75], Loss: 0.3270\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() #train or evolve\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_x, batch_y in train_dataloader:\n",
    "        batch_x = batch_x#.to(device)\n",
    "        batch_y = batch_y#.to(device)\n",
    "\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y.view(-1, 1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #directly related to the forward function defined above\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c0cd98",
   "metadata": {},
   "source": [
    "We print `mean_squared_error` and `accuracy_score` as indications of the performance of the model for regression and classification respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2351,
   "id": "dc271f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.7792207792207793\n"
     ]
    }
   ],
   "source": [
    "y_pred=model(Xte)\n",
    "# Performance metric for regression\n",
    "# print(f'MSE:{mean_squared_error(yte.detach().numpy(),y_pred.detach().numpy())}') \n",
    "\n",
    "# Performance metric for classification\n",
    "print(f'ACC:{accuracy_score(yte.detach().numpy(),y_pred.detach().numpy()>0.5)}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6259c7f6",
   "metadata": {},
   "source": [
    "## Discussion  \n",
    "\n",
    "For ``Dataset 1 (regression)``, the following MSE scores were obtained using the different models:  \n",
    "- **LS**:    2545.29  \n",
    "- **ANFIS**: 2491.41  \n",
    "- **NN**:    2818.01  \n",
    "\n",
    "We can see that the Hybrid ANFIS model achieved the lowest MSE, slightly outperforming the least squares model from assignment 1. This is because ANFIS combines least squares with gradient descent to adjust both the antecedent (fuzzy membership) and consequent parameters. The Neural Network model performed worst, likely due to insufficient tuning, leading to underfitting or poor generalization. For this dataset, a simpler model with fuzzy structure may be more effective.\n",
    "\n",
    "For ``Dataset 2 (classification)``, the following accuracy scores were obtained using the different models:  \n",
    "- **LS**:    0.7532  \n",
    "- **ANFIS**: 0.7857  \n",
    "- **NN**:    0.7792  \n",
    "\n",
    "All models performed reasonably well, with ANFIS achieving the highest accuracy, suggesting that the fuzzy clustering and rule-based structure captured class boundaries effectively. The neural network performed slightly worse than ANFIS but better than LS. In order to outperform ANFIS, it would probably require more careful tuning of architecture and hyperparameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
